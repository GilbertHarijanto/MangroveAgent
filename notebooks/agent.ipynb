{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wpXwixpWgab7",
        "mlpvuPhms5sL"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain_core langchain-openai langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJ_T3n-cVV_A",
        "outputId": "f50f6de4-a522-49f0-c191-9e123b8f858a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.5/140.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Agent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oP5xNEqJeNE6",
        "outputId": "8b585c85-fc40-490f-808d-0032854975f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Agent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", api_key=userdata.get('OPENAI_API_KEY'))"
      ],
      "metadata": {
        "id": "oNOCu2v1Vnry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xc8TEyIjVJgZ"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import Optional, Literal\n",
        "\n",
        "class QueryIntent(BaseModel):\n",
        "    goal: Literal[\"forecast\", \"research\"] = Field(\n",
        "        description=\"\"\"\n",
        "        'forecast': the user wants a contextual overview or analysis. May include NDVI forecasts and real-time data if a location is mentioned.\n",
        "        'research': the user is asking a general knowledge question not tied to a specific place or data fetch.\n",
        "        \"\"\"\n",
        "    )\n",
        "    location: Optional[str] = Field(\n",
        "        description=\"The geographic location specified in the query, if any. Return None if not found.\"\n",
        "    )\n",
        "    state: Optional[str] = Field(\n",
        "    description=\"\"\"\n",
        "    The 2-letter U.S. state abbreviation (e.g., 'FL', 'TX') associated with the location,\n",
        "    if known or inferable from the query. Helps scope station lookups regionally.\n",
        "    Return None if the query doesn't reference a U.S. state.\n",
        "    \"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "system_prompt = SystemMessage(content=\"\"\"\n",
        "You are an intelligent assistant that extracts user intent from natural language.\n",
        "\n",
        "Return:\n",
        "- goal:\n",
        "    - 'forecast' if the user is asking for an overview, condition, or analysis of mangroves, especially in a specific location.\n",
        "    - 'research' if the user asks general questions like \"What are mangroves?\" or \"Why are they important?\"\n",
        "\n",
        "- location:\n",
        "    - If the user specifies a geographic location (e.g., city, region, island, or landmark), extract it.\n",
        "    - Return None if no clear location is mentioned.\n",
        "\n",
        "- state:\n",
        "    - If the location is in the United States, extract the 2-letter state abbreviation (e.g., \"FL\" for Florida, \"LA\" for Louisiana).\n",
        "    - Return None if the location is outside the U.S. or cannot be inferred.\n",
        "\n",
        "Only use the allowed values for 'goal'. Do not guess location if it's unclear.\n",
        "\"\"\")\n",
        "\n",
        "structured_llm = llm.with_structured_output(QueryIntent)"
      ],
      "metadata": {
        "id": "Bk0VmSG4VQOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_intent_node(state: dict) -> dict:\n",
        "    user_query = state[\"user_query\"]\n",
        "\n",
        "    intent = structured_llm.invoke([system_prompt, user_query])\n",
        "\n",
        "    return {\n",
        "        \"goal\": intent.goal,\n",
        "        \"location\": intent.location,\n",
        "        \"state\": intent.state\n",
        "    }\n"
      ],
      "metadata": {
        "id": "795IklddVUXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict\n",
        "\n",
        "class State(TypedDict):\n",
        "    user_query: str\n",
        "    goal: Optional[str]\n",
        "    location: Optional[str]\n",
        "    state: Optional[str]"
      ],
      "metadata": {
        "id": "zNc5DCdWVvV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langgraph.graph import StateGraph, START\n",
        "\n",
        "# graph = StateGraph(State)\n",
        "\n",
        "# graph.add_node(\"extract_intent\", extract_intent_node)\n",
        "# graph.add_edge(START, \"extract_intent\")"
      ],
      "metadata": {
        "id": "5pW39CeNVzT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Simulate test input\n",
        "test_state_1 = {\n",
        "    \"user_query\": \"How are mangroves doing in Key West?\"\n",
        "}\n",
        "test_state_2 = {\n",
        "    \"user_query\": \"Why are mangroves important for climate change?\"\n",
        "}\n",
        "\n",
        "\n",
        "# Step 6: Run the node\n",
        "output_1 = extract_intent_node(test_state_1)\n",
        "output_2 = extract_intent_node(test_state_2)\n",
        "\n",
        "# Step 7: Print and verify output\n",
        "print(\"=== Extracted Intent ===\")\n",
        "print(f\"Output 1: {output_1}\")\n",
        "print(f\"Output 2: {output_2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzkXpZM6WIDc",
        "outputId": "25863487-5796-4c0e-8db7-35f584a9c5c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Extracted Intent ===\n",
            "Output 1: {'goal': 'summary', 'location': 'Key West', 'state': 'FL'}\n",
            "Output 2: {'goal': 'qa', 'location': None, 'state': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def geocode_location(location_name: str):\n",
        "    url = f\"https://nominatim.openstreetmap.org/search?q={location_name}&format=json\"\n",
        "    r = requests.get(url, headers={\"User-Agent\": \"LangGraph-Mangrove-Agent\"})\n",
        "    data = r.json()\n",
        "    if not data:\n",
        "        raise ValueError(f\"Could not geocode location: {location_name}\")\n",
        "    lat = float(data[0][\"lat\"])\n",
        "    lon = float(data[0][\"lon\"])\n",
        "    return (lat, lon)\n"
      ],
      "metadata": {
        "id": "xcPaApFVWIir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import radians, sin, cos, sqrt, atan2\n",
        "\n",
        "def haversine_distance(lat1, lon1, lat2, lon2):\n",
        "    R = 6371  # Earth radius in km\n",
        "    dlat = radians(lat2 - lat1)\n",
        "    dlon = radians(lon2 - lon1)\n",
        "    a = sin(dlat/2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon/2)**2\n",
        "    return R * 2 * atan2(sqrt(a), sqrt(1 - a))\n"
      ],
      "metadata": {
        "id": "0cVoLHfxaE3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scrape Turbidity Stations (USGS)"
      ],
      "metadata": {
        "id": "td6yvx7UgSGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "# import pandas as pd\n",
        "# from io import StringIO\n",
        "# import time\n",
        "\n",
        "# # All U.S. state codes\n",
        "# state_codes = [\n",
        "#     \"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \"HI\", \"ID\",\n",
        "#     \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \"MA\", \"MI\", \"MN\", \"MS\",\n",
        "#     \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\",\n",
        "#     \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\",\n",
        "#     \"WI\", \"WY\"\n",
        "# ]\n",
        "\n",
        "# base_url = \"https://waterservices.usgs.gov/nwis/site/?format=rdb&parameterCd=63680&stateCd={}\"\n",
        "\n",
        "# all_sites = []\n",
        "\n",
        "# for state in state_codes:\n",
        "#     print(f\"Fetching stations for {state}\")\n",
        "#     url = base_url.format(state)\n",
        "#     r = requests.get(url)\n",
        "\n",
        "#     # USGS RDB format: skip header lines that start with \"#\"\n",
        "#     data = \"\\n\".join(line for line in r.text.splitlines() if not line.startswith(\"#\"))\n",
        "#     df = pd.read_csv(StringIO(data), sep=\"\\t\")\n",
        "\n",
        "#     # Only keep relevant fields\n",
        "#     if not df.empty:\n",
        "#         df = df[[\"site_no\", \"station_nm\", \"dec_lat_va\", \"dec_long_va\"]]\n",
        "#         df[\"state\"] = state\n",
        "#         all_sites.append(df)\n",
        "\n",
        "#     time.sleep(1)  # be respectful of USGS servers\n",
        "\n",
        "# # Combine and save\n",
        "# turbidity_stations = pd.concat(all_sites, ignore_index=True)\n",
        "# turbidity_stations.to_csv(\"usgs_turbidity_stations.csv\", index=False)\n",
        "# print(\"✅ Saved: usgs_turbidity_stations.csv\")\n"
      ],
      "metadata": {
        "id": "fwdQi3SnaXfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf usgs_turbidity_stations.csv"
      ],
      "metadata": {
        "id": "6nIQImnAd3ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turbidity_stations = turbidity_stations[turbidity_stations['site_no']!='15s']"
      ],
      "metadata": {
        "id": "aUcCWZpSeyZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turbidity_stations = turbidity_stations.dropna(subset=[\"dec_lat_va\", \"dec_long_va\"])"
      ],
      "metadata": {
        "id": "RoAApqEEgFfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turbidity_stations = turbidity_stations.rename(columns={'station_nm':'name', 'dec_lat_va':'lat', 'dec_long_va':'long'})"
      ],
      "metadata": {
        "id": "Yci9eK9CvPLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turbidity_stations['location'] = turbidity_stations['name'].str.split(',', n=1, expand=True)[0].str.title()\n",
        "# turbidity_stations['location'] = turbidity_stations['name'].str.replace(r',.*$|, AL\\.$| AL\\.$', '', regex=True)"
      ],
      "metadata": {
        "id": "vTI0r9avwMaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turbidity_stations['name'] = turbidity_stations['location'] + ', ' + turbidity_stations['state']"
      ],
      "metadata": {
        "id": "IrEblNdkwZ5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turbidity_stations = turbidity_stations[['site_no', 'name', 'location', 'state', 'lat', 'long']]"
      ],
      "metadata": {
        "id": "4Op85Pkiwe0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turbidity_stations.head()"
      ],
      "metadata": {
        "id": "P20_bpZ3vf6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turbidity_stations.to_csv(\"turbidity_stations.csv\", index=False)"
      ],
      "metadata": {
        "id": "3Cwl123sd7Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turbidity_stations.info()"
      ],
      "metadata": {
        "id": "ULVRTdzHeAMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turbidity_stations.describe()"
      ],
      "metadata": {
        "id": "zdKelOgqeHUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turbidity_stations = turbidity_stations.rename(columns={\"site_no\":\"station_id\"})\n",
        "# turbidity_stations = turbidity_stations.drop(columns={'lat', 'long'})"
      ],
      "metadata": {
        "id": "d5ukwgA1m3HJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turbidity_stations.to_csv(\"turbidity_stations.csv\", index=None)"
      ],
      "metadata": {
        "id": "uZ-CbDKx8eaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scrape Wind Speed Stations (NOAA)"
      ],
      "metadata": {
        "id": "wpXwixpWgab7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# wind_stations = pd.read_csv('wind_speed_stations.csv')"
      ],
      "metadata": {
        "id": "bgPMzwNAeT2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wind_stations = wind_stations[['station_id', 'name']]"
      ],
      "metadata": {
        "id": "O4NYfeYfhhJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wind_stations.tail()"
      ],
      "metadata": {
        "id": "IpcAEd99mvEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wind_stations[['location', 'state']] = wind_stations['name'].str.rsplit(',', n=1, expand=True)\n",
        "\n",
        "# # Strip any whitespace from the new columns\n",
        "# wind_stations['location'] = wind_stations['location'].str.strip()\n",
        "# wind_stations['state'] = wind_stations['state'].str.strip()"
      ],
      "metadata": {
        "id": "QbL4G9fsu1BG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wind_stations = wind_stations.drop(columns={'lat', 'long'})"
      ],
      "metadata": {
        "id": "-StAlu6470Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wind_stations.head()"
      ],
      "metadata": {
        "id": "bNPii6KR770d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wind_stations.to_csv(\"wind_speed_stations_data.csv\", index=None)"
      ],
      "metadata": {
        "id": "9dhg2_Jcu02r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Install required package if not already installed\n",
        "# # !pip install geopy\n",
        "\n",
        "# import pandas as pd\n",
        "# from geopy.geocoders import Nominatim\n",
        "# from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
        "# import time\n",
        "\n",
        "# # Function to get coordinates with error handling and retry\n",
        "# def get_coordinates(location, state, max_retries=3):\n",
        "#     geolocator = Nominatim(user_agent=\"wind_station_geocoder\")\n",
        "#     search_query = f\"{location}, {state}, USA\"\n",
        "\n",
        "#     for attempt in range(max_retries):\n",
        "#         try:\n",
        "#             location_info = geolocator.geocode(search_query)\n",
        "#             if location_info:\n",
        "#                 return location_info.latitude, location_info.longitude\n",
        "#             # If no result, try with just the location name\n",
        "#             if attempt == max_retries - 1:\n",
        "#                 location_info = geolocator.geocode(location)\n",
        "#                 if location_info:\n",
        "#                     return location_info.latitude, location_info.longitude\n",
        "#             time.sleep(1)  # Respect API rate limits\n",
        "#         except (GeocoderTimedOut, GeocoderServiceError):\n",
        "#             time.sleep(2)  # Wait longer on error\n",
        "\n",
        "#     return None, None  # Return None if all attempts fail\n",
        "\n",
        "# # Apply geocoding to the DataFrame\n",
        "# def add_coordinates(df):\n",
        "#     # Create new columns for latitude and longitude\n",
        "#     df['lat'] = None\n",
        "#     df['long'] = None\n",
        "\n",
        "#     # Iterate through rows and geocode each location\n",
        "#     for idx, row in df.iterrows():\n",
        "#         lat, lng = get_coordinates(row['location'], row['state'])\n",
        "#         df.at[idx, 'lat'] = lat\n",
        "#         df.at[idx, 'long'] = lng\n",
        "#         time.sleep(1)  # Be nice to the geocoding service\n",
        "\n",
        "#         # Print progress\n",
        "#         print(f\"Geocoded: {row['location']}, {row['state']} → ({lat}, {lng})\")\n",
        "\n",
        "#     return df\n",
        "\n",
        "# # Apply to your wind_stations DataFrame\n",
        "# wind_stations_with_coords = add_coordinates(wind_stations)\n",
        "\n",
        "# # Display results\n",
        "# print(wind_stations_with_coords[['station_id', 'name', 'location', 'state', 'lat', 'long']])"
      ],
      "metadata": {
        "id": "Lm8zn-_RxKSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wind_stations_with_coords.to_csv(\"wind_stations_with_coords.csv\", index=False)\n",
        "# # turbidity_stations.to_csv(\"usgs_turbidity_stations.csv\", index=False)"
      ],
      "metadata": {
        "id": "nVNWIzfU0z-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scrape Water Level Stations (NOAA)"
      ],
      "metadata": {
        "id": "mlpvuPhms5sL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# water_stations = pd.read_csv('water_level_stations.csv')"
      ],
      "metadata": {
        "id": "YZI7H3OlnqAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# water_stations = water_stations[['station_id', 'name']]"
      ],
      "metadata": {
        "id": "pikq-rk1tGMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# water_stations = water_stations.dropna()\n",
        "# water_stations['station_id'] = water_stations['station_id'].astype(int)"
      ],
      "metadata": {
        "id": "EdCD8R3JuAP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# water_stations.head()"
      ],
      "metadata": {
        "id": "b4cssso4uTmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# water_stations.tail()"
      ],
      "metadata": {
        "id": "vUpUxDdmtInu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# water_stations[['location', 'state']] = water_stations['name'].str.rsplit(',', n=1, expand=True)\n",
        "\n",
        "# # Strip any whitespace from the new columns\n",
        "# water_stations['location'] = water_stations['location'].str.strip()\n",
        "# water_stations['state'] = water_stations['state'].str.strip()"
      ],
      "metadata": {
        "id": "Jke4xFR5tNAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# water_stations.to_csv(\"water_level_stations.csv\", index=None)"
      ],
      "metadata": {
        "id": "WrraW-2utnoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CEtAqw8KvBKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process"
      ],
      "metadata": {
        "id": "ER2Glbvq-tiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_stations(csv_path: str, state_abbr: str) -> list[dict]:\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Ensure state column exists and is properly formatted\n",
        "    if \"state\" not in df.columns:\n",
        "        raise ValueError(\"Missing 'state' column in station CSV.\")\n",
        "\n",
        "    filtered = df[df[\"state\"].str.upper() == state_abbr.upper()]\n",
        "    return filtered.to_dict(orient=\"records\")  # for LLM ranking\n"
      ],
      "metadata": {
        "id": "jS3SzH3m-vBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_llm_rank_stations(location: str, station_list: list[dict], variable: str) -> list[dict]:\n",
        "    station_names = [s['name'] for s in station_list]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    The user is asking about: {location}\n",
        "    Sensor type: {variable}\n",
        "\n",
        "    Here are some stations in the same U.S. state:\n",
        "    {station_names}\n",
        "\n",
        "    Rank the top 5 stations that are most relevant or closest to the location. Respond with a list of names (copy them exactly from the list).\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt).content\n",
        "    print(f\"=== Top 5 Nearest {variable.title()} Station ===\") # Raw LLM Output\n",
        "    print(response)\n",
        "    print()\n",
        "\n",
        "    # Normalize and remove list numbering (e.g., \"1. Station Name\")\n",
        "    import re\n",
        "    ranked_names = [\n",
        "        re.sub(r\"^\\d+\\.\\s*\", \"\", name.strip().lower())\n",
        "        for name in response.split(\"\\n\")\n",
        "        if name.strip()\n",
        "    ]\n",
        "\n",
        "    matched = []\n",
        "    for ranked_name in ranked_names:\n",
        "        for s in station_list:\n",
        "            if ranked_name in s[\"name\"].lower():\n",
        "                matched.append(s)\n",
        "                break\n",
        "    return matched[:5]\n"
      ],
      "metadata": {
        "id": "I7gT1iJq-wQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_station_by_location_node(state: dict) -> dict:\n",
        "    location = state.get(\"location\")\n",
        "    state_abbr = state.get(\"state\")\n",
        "\n",
        "    if not location or not state_abbr:\n",
        "        # Fallback to default station IDs\n",
        "        return {\n",
        "            \"station_ids\": {\n",
        "                \"wind\": \"42020\",\n",
        "                \"water\": \"8723970\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "    # Filter station lists by state\n",
        "    wind_stations = load_stations(\"wind_speed_stations.csv\", state_abbr)\n",
        "    water_stations = load_stations(\"water_level_stations.csv\", state_abbr)\n",
        "\n",
        "    # Ask LLM to rank stations by proximity to 'location'\n",
        "    ranked_wind = ask_llm_rank_stations(location, wind_stations, \"wind\")\n",
        "    ranked_water = ask_llm_rank_stations(location, water_stations, \"water level\")\n",
        "\n",
        "    return {\n",
        "        \"station_ids\": {\n",
        "            \"wind\": ranked_wind[0][\"station_id\"],\n",
        "            \"water\": ranked_water[0][\"station_id\"]\n",
        "        },\n",
        "        \"station_candidates\": {\n",
        "            \"wind\": [s[\"station_id\"] for s in ranked_wind],\n",
        "            \"water\": [s[\"station_id\"] for s in ranked_water]\n",
        "        }\n",
        "    }\n"
      ],
      "metadata": {
        "id": "R67h9ugVB7cO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stations = load_stations(\"water_level_stations.csv\", \"FL\")\n",
        "\n",
        "print(\"=== Filtered Stations ===\")\n",
        "for station in stations:\n",
        "    print(station)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJNf-TQ5-06W",
        "outputId": "df361501-6186-4642-b867-70740569e168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Filtered Stations ===\n",
            "{'station_id': 8720030, 'name': 'Fernandina Beach, FL', 'location': 'Fernandina Beach', 'state': 'FL'}\n",
            "{'station_id': 8720218, 'name': 'Mayport (Bar Pilots Dock), FL', 'location': 'Mayport (Bar Pilots Dock)', 'state': 'FL'}\n",
            "{'station_id': 8720219, 'name': 'Dames Point, FL', 'location': 'Dames Point', 'state': 'FL'}\n",
            "{'station_id': 8720226, 'name': 'Southbank Riverwalk, St Johns River, FL', 'location': 'Southbank Riverwalk, St Johns River', 'state': 'FL'}\n",
            "{'station_id': 8720357, 'name': 'I-295 Buckman Bridge, FL', 'location': 'I-295 Buckman Bridge', 'state': 'FL'}\n",
            "{'station_id': 8721604, 'name': 'Trident Pier, Port Canaveral, FL', 'location': 'Trident Pier, Port Canaveral', 'state': 'FL'}\n",
            "{'station_id': 8722670, 'name': 'Lake Worth Pier, Atlantic Ocean, FL', 'location': 'Lake Worth Pier, Atlantic Ocean', 'state': 'FL'}\n",
            "{'station_id': 8722956, 'name': 'South Port Everglades, FL', 'location': 'South Port Everglades', 'state': 'FL'}\n",
            "{'station_id': 8723214, 'name': 'Virginia Key, FL', 'location': 'Virginia Key', 'state': 'FL'}\n",
            "{'station_id': 8723970, 'name': 'Vaca Key, Florida Bay, FL', 'location': 'Vaca Key, Florida Bay', 'state': 'FL'}\n",
            "{'station_id': 8724580, 'name': 'Key West, FL', 'location': 'Key West', 'state': 'FL'}\n",
            "{'station_id': 8725114, 'name': 'NAPLES BAY, NORTH, FL', 'location': 'NAPLES BAY, NORTH', 'state': 'FL'}\n",
            "{'station_id': 8725520, 'name': 'Fort Myers, FL', 'location': 'Fort Myers', 'state': 'FL'}\n",
            "{'station_id': 8726384, 'name': 'Port Manatee, FL', 'location': 'Port Manatee', 'state': 'FL'}\n",
            "{'station_id': 8726520, 'name': 'St. Petersburg, FL', 'location': 'St. Petersburg', 'state': 'FL'}\n",
            "{'station_id': 8726607, 'name': 'Old Port Tampa, FL', 'location': 'Old Port Tampa', 'state': 'FL'}\n",
            "{'station_id': 8726674, 'name': 'East Bay, FL', 'location': 'East Bay', 'state': 'FL'}\n",
            "{'station_id': 8726724, 'name': 'Clearwater Beach, FL', 'location': 'Clearwater Beach', 'state': 'FL'}\n",
            "{'station_id': 8727520, 'name': 'Cedar Key, FL', 'location': 'Cedar Key', 'state': 'FL'}\n",
            "{'station_id': 8728690, 'name': 'Apalachicola, FL', 'location': 'Apalachicola', 'state': 'FL'}\n",
            "{'station_id': 8729108, 'name': 'Panama City, FL', 'location': 'Panama City', 'state': 'FL'}\n",
            "{'station_id': 8729210, 'name': 'Panama City Beach, FL', 'location': 'Panama City Beach', 'state': 'FL'}\n",
            "{'station_id': 8729840, 'name': 'Pensacola, FL', 'location': 'Pensacola', 'state': 'FL'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "location_query = \"Key West\"\n",
        "sensor = \"wind\"\n",
        "\n",
        "ranked = ask_llm_rank_stations(location_query, stations, sensor)\n",
        "\n",
        "print(\"\\n=== Ranked Stations ===\")\n",
        "for s in ranked:\n",
        "    print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwHAvfZE_c2a",
        "outputId": "17a157fe-1f8d-4382-8e0a-44e737ac0373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Top 5 Nearest Wind Station ===\n",
            "1. Key West, FL\n",
            "2. Vaca Key, Florida Bay, FL\n",
            "3. Virginia Key, FL\n",
            "4. South Port Everglades, FL\n",
            "5. Trident Pier, Port Canaveral, FL\n",
            "\n",
            "\n",
            "=== Ranked Stations ===\n",
            "{'station_id': 8724580, 'name': 'Key West, FL', 'location': 'Key West', 'state': 'FL'}\n",
            "{'station_id': 8723970, 'name': 'Vaca Key, Florida Bay, FL', 'location': 'Vaca Key, Florida Bay', 'state': 'FL'}\n",
            "{'station_id': 8723214, 'name': 'Virginia Key, FL', 'location': 'Virginia Key', 'state': 'FL'}\n",
            "{'station_id': 8722956, 'name': 'South Port Everglades, FL', 'location': 'South Port Everglades', 'state': 'FL'}\n",
            "{'station_id': 8721604, 'name': 'Trident Pier, Port Canaveral, FL', 'location': 'Trident Pier, Port Canaveral', 'state': 'FL'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "GWHJYH5s_s8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_wind_speed(station_id: str) -> Optional[float]:\n",
        "    url = \"https://api.tidesandcurrents.noaa.gov/api/prod/datagetter\"\n",
        "    params = {\n",
        "        \"date\": \"latest\",\n",
        "        \"station\": station_id,\n",
        "        \"product\": \"wind\",\n",
        "        \"units\": \"english\",\n",
        "        \"time_zone\": \"gmt\",\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, params=params, timeout=5)\n",
        "        if not r.ok:\n",
        "            print(f\"[{station_id}] Bad response: {r.status_code}\")\n",
        "            return None\n",
        "\n",
        "        data = r.json()\n",
        "        if \"data\" not in data or not data[\"data\"]:\n",
        "            print(f\"[{station_id}] No 'data' in wind response\")\n",
        "            return None\n",
        "\n",
        "        wind_speed = float(data[\"data\"][0][\"s\"])\n",
        "        return wind_speed\n",
        "    except Exception as e:\n",
        "        print(f\"[{station_id}] Wind speed fetch error: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "adC0b1KTAvTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_water_level(station_id: str) -> Optional[float]:\n",
        "    url = \"https://api.tidesandcurrents.noaa.gov/api/prod/datagetter\"\n",
        "    params = {\n",
        "        \"date\": \"latest\",\n",
        "        \"station\": station_id,\n",
        "        \"product\": \"water_level\",\n",
        "        \"datum\": \"MLLW\",\n",
        "        \"units\": \"english\",\n",
        "        \"time_zone\": \"gmt\",\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        r = requests.get(url, params=params, timeout=5)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        if \"data\" in data and data[\"data\"]:\n",
        "            return float(data[\"data\"][0][\"v\"])\n",
        "        print(f\"[{station_id}] No data in response.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"[{station_id}] Water level fetch error: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "8mxLKCFNAxpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def try_fetch_with_fallback(fetch_func, candidate_ids: list[str]) -> Optional[float]:\n",
        "    for station_id in candidate_ids:\n",
        "        result = fetch_func(station_id)\n",
        "        if result is not None:\n",
        "            return result\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "CVddAfy4AzcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_environmental_data_node(state: dict) -> dict:\n",
        "    stations = state[\"station_ids\"]\n",
        "    candidates = state.get(\"station_candidates\", {})\n",
        "\n",
        "    wind = try_fetch_with_fallback(fetch_wind_speed, candidates.get(\"wind\", [stations[\"wind\"]]))\n",
        "    water = try_fetch_with_fallback(fetch_water_level, candidates.get(\"water\", [stations[\"water\"]]))\n",
        "\n",
        "    return {\n",
        "        \"environmental_data\": {\n",
        "            \"wind_speed\": wind,\n",
        "            \"water_level\": water\n",
        "        }\n",
        "    }\n"
      ],
      "metadata": {
        "id": "FuAF5W02A2b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Known tide station\n",
        "print(fetch_wind_speed(\"8723970\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_PbP6YEBNXi",
        "outputId": "df73a8ce-5596-49b0-8493-0954ce16fd68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ee\n",
        "import datetime\n",
        "\n",
        "ee.Authenticate()\n",
        "\n",
        "# Initialize with your project\n",
        "ee.Initialize(project='ee-lgharijanto123')\n",
        "\n",
        "def get_cleaned_weekly_ndvi_series(lat: float, lon: float, days_back: int = 70) -> list[float]:\n",
        "    point = ee.Geometry.Point([lon, lat])\n",
        "    study_area = point.buffer(16000)\n",
        "\n",
        "    today = datetime.date.today()\n",
        "    start = ee.Date(today.strftime('%Y-%m-%d')).advance(-days_back, 'day')\n",
        "\n",
        "    def add_ndvi(image):\n",
        "        ndvi = image.normalizedDifference(['sur_refl_b02', 'sur_refl_b01']).rename('NDVI')\n",
        "        return image.addBands(ndvi)\n",
        "\n",
        "    def extract_mean(image):\n",
        "        mean = image.select('NDVI').reduceRegion(\n",
        "            reducer=ee.Reducer.mean(),\n",
        "            geometry=study_area,\n",
        "            scale=500,\n",
        "            maxPixels=1e9,\n",
        "            bestEffort=True\n",
        "        ).get('NDVI')\n",
        "        return ee.Feature(None, {'ndvi': mean})\n",
        "\n",
        "    ndvi_series = (\n",
        "        ee.ImageCollection('MODIS/061/MOD09GA')\n",
        "        .filterBounds(study_area)\n",
        "        .filterDate(start, ee.Date(today.strftime('%Y-%m-%d')))\n",
        "        .map(add_ndvi)\n",
        "        .map(extract_mean)\n",
        "        .filter(ee.Filter.notNull(['ndvi']))\n",
        "    )\n",
        "\n",
        "    ndvi_list = ndvi_series.aggregate_array('ndvi').getInfo()\n",
        "    ndvi_floats = [v / 10000 if v > 1 else v for v in ndvi_list]\n",
        "\n",
        "    if len(ndvi_floats) < 56:\n",
        "        raise ValueError(f\"Not enough data. Got {len(ndvi_floats)} daily values, need at least 56.\")\n",
        "\n",
        "    weekly_ndvi = []\n",
        "    for i in range(0, len(ndvi_floats) - 56 + 56, 7):\n",
        "        chunk = ndvi_floats[i:i+7]\n",
        "        if len(chunk) == 7:\n",
        "            weekly_ndvi.append(sum(chunk) / 7)\n",
        "\n",
        "    if len(weekly_ndvi) < 8:\n",
        "        raise ValueError(f\"Only {len(weekly_ndvi)} weekly values found, need 8.\")\n",
        "\n",
        "    return weekly_ndvi[-8:]\n"
      ],
      "metadata": {
        "id": "_GHuCMFpEqAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vals = get_cleaned_weekly_ndvi_series(25.9928, -81.3923)  # Pumpkin River\n",
        "print(vals)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7mfvHffMVSa",
        "outputId": "d66848aa-7d04-4b12-c848-51a15ef31118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.4546419024603802, 0.5214106775183016, 0.32694591497867, 0.3648745033678752, 0.450423371305643, 0.5419387288275995, 0.5322891062851883, 0.3217713250057927]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_weekly_noaa_lags_chunked(station_id: str, product: str, total_days: int = 56) -> list[float]:\n",
        "    from datetime import datetime, timedelta\n",
        "    import requests\n",
        "    import pandas as pd\n",
        "\n",
        "    end = datetime.utcnow().date()\n",
        "    start = end - timedelta(days=total_days)\n",
        "\n",
        "    # Break range into 3 chunks (e.g., 18 + 19 + 19 days)\n",
        "    chunk_starts = [start + timedelta(days=i * 18) for i in range(3)]\n",
        "    chunk_ends = [min(s + timedelta(days=18), end) for s in chunk_starts]\n",
        "\n",
        "    all_dfs = []\n",
        "\n",
        "    for chunk_start, chunk_end in zip(chunk_starts, chunk_ends):\n",
        "        params = {\n",
        "            \"begin_date\": chunk_start.strftime(\"%Y%m%d\"),\n",
        "            \"end_date\": chunk_end.strftime(\"%Y%m%d\"),\n",
        "            \"station\": station_id,\n",
        "            \"product\": product,\n",
        "            \"datum\": \"MLLW\" if product == \"water_level\" else None,\n",
        "            \"interval\": \"h\",  # Hourly granularity\n",
        "            \"units\": \"english\",\n",
        "            \"time_zone\": \"gmt\",\n",
        "            \"format\": \"json\"\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            r = requests.get(\"https://api.tidesandcurrents.noaa.gov/api/prod/datagetter\",\n",
        "                             params={k: v for k, v in params.items() if v is not None},\n",
        "                             timeout=15)\n",
        "            r.raise_for_status()\n",
        "            data = r.json().get(\"data\", [])\n",
        "            if not data:\n",
        "                continue\n",
        "\n",
        "            df = pd.DataFrame(data)\n",
        "            df[\"t\"] = pd.to_datetime(df[\"t\"])\n",
        "            df.set_index(\"t\", inplace=True)\n",
        "\n",
        "            value_col = \"s\" if product == \"wind\" else \"v\"\n",
        "            df[value_col] = pd.to_numeric(df[value_col], errors=\"coerce\")\n",
        "\n",
        "            all_dfs.append(df)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Chunk {chunk_start}–{chunk_end} failed: {e}\")\n",
        "\n",
        "    if not all_dfs:\n",
        "        print(f\"No valid data collected for {product}.\")\n",
        "        return []\n",
        "\n",
        "    full_df = pd.concat(all_dfs).sort_index()\n",
        "\n",
        "    # Resample into weekly means (week ends Sunday by default)\n",
        "    value_col = \"s\" if product == \"wind\" else \"v\"\n",
        "    weekly = full_df[value_col].resample(\"W\").mean().dropna()\n",
        "\n",
        "    if len(weekly) < 8:\n",
        "        print(f\"Only {len(weekly)} weekly values found for {product}, expected 8.\")\n",
        "        return weekly.tolist()  # Return whatever we have\n",
        "\n",
        "    return weekly.tail(8).tolist()\n"
      ],
      "metadata": {
        "id": "Y4IqXjhQKMsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wind_lags = fetch_weekly_noaa_lags_chunked(\"8724580\", \"wind\")\n",
        "water_lags = fetch_weekly_noaa_lags_chunked(\"8724580\", \"water_level\")\n",
        "\n",
        "print(\"WIND LAGS:\", wind_lags)\n",
        "print(\"WATER LAGS:\", water_lags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfV4jPjKQcPA",
        "outputId": "28fa6955-72aa-476b-af73-0aac40535137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WIND LAGS: [8.901309523809525, 8.817380952380953, 9.440729166666667, 8.936190476190475, 8.258802083333334, 9.368333333333332, 8.63452380952381, 4.536041666666667]\n",
            "WATER LAGS: [0.9006303571428571, 0.8182613095238096, 1.23991875, 1.1934892857142856, 1.0897671875000001, 0.9026880952380952, 1.023640476190476, 1.1889354166666666]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def build_feature_vector_node(state: dict) -> dict:\n",
        "    try:\n",
        "        # Fetch 8 weeks (current + 7 lags)\n",
        "        wind_vals = fetch_weekly_noaa_lags_chunked(state[\"station_ids\"][\"wind\"], \"wind\")\n",
        "        water_vals = fetch_weekly_noaa_lags_chunked(state[\"station_ids\"][\"water\"], \"water_level\")\n",
        "        ndvi_vals = get_cleaned_weekly_ndvi_series(*state[\"gps\"])\n",
        "\n",
        "        if len(wind_vals) < 8 or len(water_vals) < 8 or len(ndvi_vals) < 8:\n",
        "            print(\"Insufficient weekly data.\")\n",
        "            return {\"feature_vector\": [], \"feature_df\": None}\n",
        "\n",
        "        # Build DataFrame (already in chronological order: oldest → newest)\n",
        "        df = pd.DataFrame({\n",
        "            \"tide_verified\": water_vals,\n",
        "            \"wind_speed\": wind_vals,\n",
        "            \"ndvi\": ndvi_vals\n",
        "        })\n",
        "\n",
        "        # Add artificial weekly dates (most recent week = today)\n",
        "        base_date = pd.to_datetime(\"today\").normalize()\n",
        "        df[\"date\"] = [base_date - pd.Timedelta(weeks=i) for i in reversed(range(len(df)))]\n",
        "        df.set_index(\"date\", inplace=True)\n",
        "\n",
        "        # Add lag features (1–7)\n",
        "        for col in [\"tide_verified\", \"wind_speed\", \"ndvi\"]:\n",
        "            for lag in range(1, 8):\n",
        "                df[f\"{col}_lag_{lag}\"] = df[col].shift(lag)\n",
        "\n",
        "        # Drop NaNs → only final row will be complete\n",
        "        latest_row = df.dropna().iloc[-1]\n",
        "\n",
        "        # Select 21 lag features in model training order\n",
        "        # Get current values first\n",
        "        features = latest_row[[\"tide_verified\", \"wind_speed\"]].tolist()\n",
        "\n",
        "        # Add lags\n",
        "        features += latest_row[\n",
        "            [f\"{col}_lag_{i}\" for col in [\"tide_verified\", \"wind_speed\", \"ndvi\"] for i in range(1, 8)]\n",
        "        ].tolist()\n",
        "\n",
        "        print(\"[feature vector]\", features)\n",
        "\n",
        "        return {\n",
        "            \"feature_vector\": features,\n",
        "            \"feature_df\": df\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Feature vector error:\", e)\n",
        "        return {\"feature_vector\": [], \"feature_df\": None}\n"
      ],
      "metadata": {
        "id": "j3U7nPY0PNyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "# Load once at the top level (recommended)\n",
        "xgb_model = joblib.load(\"xgboost.pkl\")\n",
        "scaler = joblib.load(\"scaler.pkl\")\n",
        "\n",
        "def predict_ndvi_node(state: dict) -> dict:\n",
        "    try:\n",
        "        features = state.get(\"feature_vector\")\n",
        "        if not features or len(features) != 23:\n",
        "            print(\"Invalid or missing feature vector\")\n",
        "            return {\"ndvi_prediction\": None}\n",
        "\n",
        "        X = pd.DataFrame([features], columns=scaler.feature_names_in_)\n",
        "        # print(\"Raw input:\", X)\n",
        "\n",
        "        # Standardize using the saved training scaler\n",
        "        X_scaled = scaler.transform(X)\n",
        "        # print(\"Scaled input:\", X_scaled)\n",
        "\n",
        "        pred = xgb_model.predict(X_scaled)[0]\n",
        "        print(\"NDVI Prediction:\", pred)\n",
        "\n",
        "        return {\"ndvi_prediction\": float(pred)}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Prediction error:\", e)\n",
        "        return {\"ndvi_prediction\": None}\n"
      ],
      "metadata": {
        "id": "Ttj1i6Cf7LIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import Runnable\n",
        "\n",
        "summary_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a coastal ecology assistant.\n",
        "\n",
        "Given the following data:\n",
        "- User question: {user_query}\n",
        "- NDVI prediction: {ndvi_prediction}\n",
        "- Wind speed: {wind_speed}\n",
        "- Water level: {water_level}\n",
        "\n",
        "Generate a clear, concise 5-6 sentence summary of the mangrove condition and any notable environmental trends. Always include all the numbers provided.\n",
        "\"\"\")\n",
        "\n",
        "summary_chain: Runnable = summary_prompt | llm  # assumes your LLM is available\n",
        "\n",
        "def generate_summary_node(state: State) -> dict:\n",
        "    try:\n",
        "        return {\n",
        "            \"summary\": summary_chain.invoke({\n",
        "                \"user_query\": state[\"user_query\"],\n",
        "                \"ndvi_prediction\": state[\"ndvi_prediction\"],\n",
        "                \"wind_speed\": state[\"environmental_data\"][\"wind_speed\"],\n",
        "                \"water_level\": state[\"environmental_data\"][\"water_level\"]\n",
        "            })\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(\"[summary] Error:\", e)\n",
        "        return {\"summary\": \"Unable to generate summary.\"}"
      ],
      "metadata": {
        "id": "gwFLtx4DSzIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resolve_gps_from_location_node(state: dict) -> dict:\n",
        "    location = state.get(\"location\")\n",
        "    state_abbr = state.get(\"state\")\n",
        "\n",
        "    print(f\"[resolve_gps] location={location}, state={state_abbr}\")\n",
        "\n",
        "    if not location:\n",
        "        print(\"[resolve_gps] Missing location.\")\n",
        "        return {\"gps\": None}\n",
        "\n",
        "    try:\n",
        "        query = location if not state_abbr else f\"{location}, {state_abbr}\"\n",
        "        lat, lon = geocode_location(query)\n",
        "        print(f\"[resolve_gps] Geocoded → {lat}, {lon}\")\n",
        "        return {\"gps\": (lat, lon)}\n",
        "    except Exception as e:\n",
        "        print(\"[resolve_gps] Error:\", e)\n",
        "        return {\"gps\": None}\n"
      ],
      "metadata": {
        "id": "6M7rF4ejQPV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_ndvi_lags_node(state: dict) -> dict:\n",
        "    gps = state.get(\"gps\")\n",
        "    if not gps:\n",
        "        print(\"[ndvi lags] GPS missing\")\n",
        "        return {\"ndvi_lags\": []}\n",
        "    lat, lon = gps\n",
        "    ndvi_lags = get_cleaned_weekly_ndvi_series(lat, lon)[:7]\n",
        "    print(\"[ndvi lags]\", ndvi_lags)\n",
        "    return {\"ndvi_lags\": ndvi_lags}\n"
      ],
      "metadata": {
        "id": "2dy2R0pYQPn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_weekly_lags_node(state: dict) -> dict:\n",
        "    stations = state.get(\"station_ids\", {})\n",
        "    wind = fetch_weekly_noaa_lags_chunked(stations.get(\"wind\"), \"wind\")\n",
        "    water = fetch_weekly_noaa_lags_chunked(stations.get(\"water\"), \"water_level\")\n",
        "    print(\"[wind lags]\", wind)\n",
        "    print(\"[water lags]\", water)\n",
        "    return {\"wind_lags\": wind, \"water_lags\": water}\n"
      ],
      "metadata": {
        "id": "-cNaWuz3QRZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Optional, Tuple, Dict, List\n",
        "\n",
        "class State(TypedDict):\n",
        "    # === User input & intent extraction ===\n",
        "    user_query: str                             # Always provided by the user\n",
        "    goal: Optional[str]                         # 'forecast' or 'research'\n",
        "    location: Optional[str]                     # e.g. \"Key West\"\n",
        "    state: Optional[str]                        # e.g. \"FL\"\n",
        "\n",
        "    # === Location resolution ===\n",
        "    gps: Optional[Tuple[float, float]]          # (lat, lon)\n",
        "\n",
        "    # === Station selection ===\n",
        "    station_ids: Optional[Dict[str, str]]       # {wind: id, water: id}\n",
        "    station_candidates: Optional[Dict[str, List[str]]]  # fallback ids\n",
        "\n",
        "    # === Real-time fetches\n",
        "    environmental_data: Optional[Dict[str, float]]  # {'wind_speed': val, 'water_level': val}\n",
        "\n",
        "    # === Weekly history (optional: for inspection only)\n",
        "    wind_lags: Optional[List[float]]\n",
        "    water_lags: Optional[List[float]]\n",
        "    ndvi_lags: Optional[List[float]]\n",
        "\n",
        "\n",
        "    # === Feature engineering ===\n",
        "    feature_vector: Optional[List[float]]       # Final model input\n",
        "    feature_df: Optional[\"pd.DataFrame\"]        # Optional for debugging\n",
        "\n",
        "    # === Model output ===\n",
        "    ndvi_prediction: Optional[float]\n",
        "    summary: Optional[str]\n"
      ],
      "metadata": {
        "id": "JmmNjxrtIfVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# Initialize your graph\n",
        "workflow = StateGraph(State)\n",
        "\n",
        "# Register all nodes before compile\n",
        "workflow.add_node(\"extract_intent\", extract_intent_node)\n",
        "workflow.add_node(\"select_stations\", select_station_by_location_node)\n",
        "workflow.add_node(\"fetch_environmental_data\", fetch_environmental_data_node)\n",
        "workflow.add_node(\"resolve_gps\", resolve_gps_from_location_node)\n",
        "workflow.add_node(\"fetch_ndvi_lags\", fetch_ndvi_lags_node)\n",
        "workflow.add_node(\"fetch_weekly_lags\", fetch_weekly_lags_node)\n",
        "workflow.add_node(\"build_feature_vector\", build_feature_vector_node)\n",
        "workflow.add_node(\"predict_ndvi\", predict_ndvi_node)\n",
        "workflow.add_node(\"generate_summary\", generate_summary_node)\n",
        "\n",
        "# Connect the nodes\n",
        "workflow.set_entry_point(\"extract_intent\")\n",
        "workflow.add_edge(\"extract_intent\", \"select_stations\")\n",
        "workflow.add_edge(\"select_stations\", \"fetch_environmental_data\")\n",
        "workflow.add_edge(\"select_stations\", \"resolve_gps\")\n",
        "workflow.add_edge(\"select_stations\", \"fetch_weekly_lags\")\n",
        "workflow.add_edge(\"resolve_gps\", \"fetch_ndvi_lags\")\n",
        "workflow.add_edge(\"fetch_weekly_lags\", \"build_feature_vector\")\n",
        "workflow.add_edge(\"fetch_ndvi_lags\", \"build_feature_vector\")\n",
        "workflow.add_edge(\"build_feature_vector\", \"predict_ndvi\")\n",
        "workflow.add_edge(\"predict_ndvi\", \"generate_summary\")\n",
        "workflow.add_edge(\"generate_summary\", END)\n",
        "\n",
        "# Finalize the graph\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "KxlKHssUQYel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = app.invoke({\"user_query\": \"How are mangroves doing in Key West?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkE3L1wBR--F",
        "outputId": "d52158ab-0b34-4933-8989-ed7a21f88047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Top 5 Nearest Wind Station ===\n",
            "1. Vaca Key, Florida Bay, FL\n",
            "2. Key West, FL\n",
            "3. Fort Myers, FL\n",
            "4. Port Manatee, FL\n",
            "5. Middle Tampa Bay, FL\n",
            "\n",
            "=== Top 5 Nearest Water Level Station ===\n",
            "1. Key West, FL\n",
            "2. Vaca Key, Florida Bay, FL\n",
            "3. Virginia Key, FL\n",
            "4. South Port Everglades, FL\n",
            "5. Trident Pier, Port Canaveral, FL\n",
            "\n",
            "[resolve_gps] location=Key West, state=FL\n",
            "[resolve_gps] Geocoded → 24.5548262, -81.8020722\n",
            "[wind lags] [6.448095238095238, 7.748333333333333, 6.576614583333334, 7.612083333333333, 6.287239583333334, 7.427142857142857, 6.8218452380952375, 3.656875]\n",
            "[water lags] [0.9006303571428571, 0.8182613095238096, 1.23991875, 1.1934892857142856, 1.0897671875000001, 0.9026880952380952, 1.023640476190476, 1.1889354166666666]\n",
            "[ndvi lags] [-0.29172347926953784, -0.2507707741115039, -0.39723508979789457, -0.17780758686841117, -0.16835179260007754, -0.3476457947141554, -0.3137031269951354]\n",
            "[feature vector] [1.1889354166666666, 3.656875, 1.023640476190476, 0.9026880952380952, 1.0897671875000001, 1.1934892857142856, 1.23991875, 0.8182613095238096, 0.9006303571428571, 6.8218452380952375, 7.427142857142857, 6.287239583333334, 7.612083333333333, 6.576614583333334, 7.748333333333333, 6.448095238095238, -0.3137031269951354, -0.3476457947141554, -0.16835179260007754, -0.17780758686841117, -0.39723508979789457, -0.2507707741115039, -0.29172347926953784]\n",
            "NDVI Prediction: 0.4057976\n",
            "[feature vector] [1.1889354166666666, 3.656875, 1.023640476190476, 0.9026880952380952, 1.0897671875000001, 1.1934892857142856, 1.23991875, 0.8182613095238096, 0.9006303571428571, 6.8218452380952375, 7.427142857142857, 6.287239583333334, 7.612083333333333, 6.576614583333334, 7.748333333333333, 6.448095238095238, -0.3137031269951354, -0.3476457947141554, -0.16835179260007754, -0.17780758686841117, -0.39723508979789457, -0.2507707741115039, -0.29172347926953784]\n",
            "NDVI Prediction: 0.4057976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['summary'].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4p_xBBr9CEmw",
        "outputId": "62457384-581d-4c19-866c-c875928cd85a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In Key West, the current NDVI prediction for mangroves stands at 0.41, indicating moderate vegetation health. The wind speed is 9.52 mph, and the water level is at 2.052 feet. These conditions suggest stable, but not optimal, growth for mangroves, with environmental factors remaining within a typical range.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AOoU-6AUCrHI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}